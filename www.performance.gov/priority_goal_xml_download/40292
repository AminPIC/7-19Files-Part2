<?xml version="1.0" encoding="UTF-8" ?><data><Agency_Priority_Goal><Agency_ID>25</Agency_ID><Agency_Name>Department of Education</Agency_Name><Agency_Priority_Goal_ID>40292</Agency_Priority_Goal_ID><Fiscal_Year>FY16-17</Fiscal_Year><APG_Header>Enable evidence-based decision making </APG_Header><APG_Statement>Increase use and generation of credible evidence on what works and what does not work in education. By September 30, 2017, ED will increase to 20% the percentage of new competitive grant dollars that support evidence-based strategies. By September 30, 2017, ED will increase by 20 the number of ED-funded project evaluations that provide credible evidence about what works in education.
</APG_Statement><APG_Overview>GOAL OVERVIEW

&amp;nbsp;

The Department of Education (ED) is committed to using its resources strategically to increase the amount of rigorous evidence about what works in education. Education leaders need this evidence to decide whether a potential program, policy, or practice is likely to produce improved student or other educational outcomes. While ED supports research on what works through its research programs, it also is committed to taking every opportunity to build and use research evidence in a range of competitive grant programs.

&amp;nbsp;

ED takes a two-pronged approach to evidence-based grant making with its competitive grants.&amp;nbsp; First, ED directs its competitive grant dollars to approaches that are supported by at least some evidence. Second, ED generates new evidence by asking grantees to conduct rigorous evaluations of their interventions.

&amp;nbsp;

ED’s first competitive grant program to use an evidence-based grant making strategy (other than research programs at the Institute of Education Sciences (IES)) was the Investing in Innovation program (i3), which made its first grants in 2010 and currently manages a portfolio of over 100 projects. Since then, ED has expanded its evidence-based grant making to other competitive grant programs when appropriate, given resources and program purpose. The First in the World program (FITW) is an example of a postsecondary program that invests in approaches with at least some research support and asks grantees to conduct rigorous studies of impact on student outcomes. FITW made new evidence-based awards in FYs 14 and 15, but was not funded in FY16.

&amp;nbsp;

Doing evidence-based grant making well involves substantial, coordinated effort from many ED offices. ED must assess whether there is existing evidence on which applicants can build, help applicants to understand ED’s evidence standards, support applicants’ search for and application of evidence, coordinate timelines and logistics of internal study reviews (to ensure that applicants who cite evidence in support of their proposed projects are citing appropriately rigorous studies), and provide support on grantees’ rigorous evaluations. ED learned from the i3 experience that technical support for grantees and their evaluators is crucial to producing the highest-caliber evidence. By way of example, ED’s application of high-quality support for grantees’ evaluations has ensured that, to date, 22 i3 projects have met rigorous What Works Clearinghouse (WWC) Evidence Standards, 107 i3 projects are currently expected to meet those standards, and 40 of 42 FITW projects are currently expected to meet them. However, providing support for rigorous studies in a consistent, cost-effective way is a new challenge that ED faces as it rolls out rigorous study expectations to additional grant programs.

&amp;nbsp;

Evidence-based grant making has been a high-priority initiative at ED for the past several years, and ED has underscored its importance more recently with the FY14 – FY15 APG focused on increasing the percentage of competitive dollars that support evidence-based strategies.&amp;nbsp; Prior to FY15, we focused intently on scaling the successful practices of i3 to other competitive grant programs. In FY15, ED shifted its focus to (1) investing our resources more carefully to ensure that the quality of implementing these strategies remains strong, and (2) picking only those programs where the evidence-based grant making approaches described above are likely to be successful.

&amp;nbsp;

Given ED’s current competitive grant programs, the availability of evidence in the field, the current funding and statutory landscape, and early projections for program funding over the next few years, increasing the new funding that supports evidence-based practices to 18% by the end of FY16 and 20% by the end of FY17 is an ambitious, yet achievable, goal. As background, 9% of ED’s new competitive grant funding supported evidence-based practices in FY12. As of FY14, almost 16% of ED’s new competitive grant funding supported evidence-based practices, and the FY15 percentage, which serves as the baseline, was significantly higher still – over 29%. Some of this jump is attributable to the size of competitions using evidence in FY15, and the number of applicants in those competitions. We do not assume this percentage will grow at a linear rate over time, and note that when taking into account ED’s ongoing planning process for using evidence in its competitive programs, and ED’s appropriations for FY16, it is likely that ensuring that more than 29% of its competitive dollars support evidence-based activities may not be possible or wise. We believe that ED has scaled this work to almost all of the competitive programs for which it makes sense to do so, and are focusing our next two years on quality of implementation so that use of evidence in competitions is supported and implemented in a meaningful way.

&amp;nbsp;

The second metric of the APG focuses on the number of rigorous studies ED expects to add to the education research base. ED expects that i3, in particular, will add a large number of studies on what works for elementary and secondary students.&amp;nbsp;Over the next few years, we expect that 42 i3 grantees will release studies that meet WWC Evidence Standards. Because the timing of when those studies will be published and receive an official WWC rating varies, and because some i3 grantees have received extensions to collect and analyze more data, we estimated that 20 of these studies will be added to the WWC database&amp;nbsp;and&amp;nbsp;will be determined to meet WWC Evidence Standards by the end of FY17.&amp;nbsp;As of the end of FY16, we have surpassed this target and 22 studies from i3 grantees were found to meet WWC Evidence Standards.&amp;nbsp; Since the WWC began releasing evidence reviews in 2006, more than 2,350 eligible studies (effectiveness studies with an outcome identified in a review protocol) have been reviewed.&amp;nbsp; Of those, 878 studies (approximately 37%) have findings that meet WWC standards with or without reservations.&amp;nbsp; i3’s contribution, over the next two years, of 20 rigorous effectiveness studies is critical as educators, families, and policymakers continue to seek clear and credible information on what works in education.

In addition to i3, ED supports a focus on rigorous evaluation in other competitive grant programs, such as Supporting Effective Educator Development (SEED) and FITW. However,&amp;nbsp;because i3 will be the only program to contribute studies by the end of FY17, only i3 project evaluations are included in the formal reporting on this APG.&amp;nbsp;Even so, ED intends to provide future-focused updates that address ED’s work to support rigorous research endeavors in other programs.&amp;nbsp;

&amp;nbsp;

As noted above, it is important that grantees tasked with rigorously evaluating their projects receive high-quality technical assistance in order to produce credible data. In addition to counting the rigorous studies that ED competitive grant programs support, this APG speaks to ED’s ability&amp;nbsp;to support, through funding and technical support, high-quality evaluation.

&amp;nbsp;

&amp;nbsp;

KEY BARRIERS AND CHALLENGES

&amp;nbsp;

Using evidence to inform competitive grant funding decisions entails a shift in culture and capacity across ED, yet ED has limited resources to support those program offices in doing this work well. Final appropriations and other funding decisions and trade-offs also influence the amounts available to competitive grant programs. For example, the FITW program is not funded in FY16. If this and other programs that ED currently considers to be evidence-based are not funded in FY17, it may be more challenging to meet the established targets. In addition, in FY17 ED will begin implementing the Every Student Succeeds Act (ESSA), which reauthorizes the Elementary and Secondary Education Act of 1965 and includes many changes, large and small, for grant programs. ESSA’s emphasis on the use of “evidence-based” activities creates an opportunity to increase funding that supports evidence-based activities in the future, but the challenges of implementing the new law may affect this APG in the short term while ED works to help the field understand the new definition and to support grantees in meeting new evidence requirements.

&amp;nbsp;

In Q4, ED announced the winners for all but two grant competitions that contribute to this APG – these programs have funds available beyond September 30, 2016.&amp;nbsp; In general, however, we note that ED’s focus on transitioning to ESSA has created a relative dearth of staff capacity across many of its competitive grant programs.&amp;nbsp; While we spent FY16 competitive dollars within the timeframes mandated by the Congress, delays in a competition schedule from previous quarters caused us to truncate crucial tasks, such as reviewing the evidence applicants submit in support of their projects.&amp;nbsp; ED continues to learn from past evidence-based competitions to improve our processes.

&amp;nbsp;

In addition, supporting grantees as they rigorously evaluate their grant-funded projects is resource-intensive and difficult to do well.&amp;nbsp;Technical assistance is costly, and many small programs, or programs funded under different authorities, have limited ability to provide the assistance that grantees need to stay on track.&amp;nbsp;All rigorous studies of effectiveness require careful stewardship, and even in the hands of skilled evaluators, many things can go wrong in the implementation of a study that threaten its credibility. Further, unlike research grant programs, competitive grant programs consider the qualifications of evaluators as one factor among many, with the result that not all evaluators of funded projects have significant experience in conducting rigorous studies of effectiveness. &amp;nbsp;ED continues to problem solve with specific program offices in order to apply the i3 model in an accessible way to get more credible and informative data from grantees. ED’s Evidence Planning Group (EPG) continues to keep this issue on its radar, but focused in Q4 on closing out FY16 competitions and preparing for select FY17 competitions.

&amp;nbsp;

In particular, as elementary and secondary programs transition to the ESEA as amended by the ESSA, new statutory provisions that require or incentivize evidence-based practices have caused hiccups in FY17 planning.&amp;nbsp; Specifically, the ESEA’s definition for “evidence-based” aligns with, but does not match exactly, the definitions ED created through regulations in 2013, and certain differences have created substantial process complications.&amp;nbsp; EPG devoted ample time in Q4 to come up with a short-term solution for FY17, and a longer term solution for future years, that ensures that ED continues to hold its applicants to rigorous standards while still considering the internal capacity of staff to conduct evidence-based grant competitions with integrity.

&amp;nbsp;

In addition, there are several programs for which ED’s model of evidence-based grant making is problematic or not possible. Some programs provide funding to comprehensive support centers that respond to the needs of an education community – it is difficult to require that all grantees under such a program demonstrate that they will only provide evidence-based support when the applicant may not be able to predict the challenges that will arise in the community or anticipate the amount of research available on such challenges.

&amp;nbsp;

Finally, despite general support for evidence-based grant making endeavors, many programs’ stakeholders, who are accustomed to the status quo, have pushed back on integrating evidence into competitive grant programs. Long-standing funding levels and competition designs can make applicants less willing to move toward evidence-based strategies that require them to evaluate their work’s overall impact.

&amp;nbsp;

EXTERNAL STAKEHOLDERS

There is an increasing emphasis among stakeholders on the importance of using evidence to support government program funding decisions, and ED regularly engages the field on this topic. A number of outside organizations have convened experts to discuss how to encourage such decisions. In addition, philanthropic and congressional actors prioritized using evidence to support decision-making and have encouraged the field to do the same. Finally, ED has worked with the National Science Foundation to develop a common evidence framework around which to organize research investments and grants.
</APG_Overview><Goal_Leaders><Goal_Leader><goal_leader_name>Amy McIntosh</goal_leader_name><goal_leader_title>Acting Assistant Secretary</goal_leader_title><goal_leader_org>Office of Planning, Evaluation and Policy Development</goal_leader_org></Goal_Leader><Goal_Leader><goal_leader_name>Margo Anderson</goal_leader_name><goal_leader_title>Associate Assistant Deputy Secretary</goal_leader_title><goal_leader_org>Office of Innovation and Improvement</goal_leader_org></Goal_Leader><Goal_Leader><goal_leader_name>Ruth Neild</goal_leader_name><goal_leader_title>Delegated Duties of the Director</goal_leader_title><goal_leader_org>Institute of Education Sciences </goal_leader_org></Goal_Leader></Goal_Leaders><next_steps>NEXT STEPS

&amp;nbsp;

EPG continues to work with program offices managing evidence-based competitions to provide resources in such areas as the existing research on particular topics and guidance on competition design. &amp;nbsp;&amp;nbsp;ED uses two indicators to determine whether we are on track to meet or surpass our target for the year:&amp;nbsp; (1) documented intentions from programs that are running a new competition and planning to use evidence in those competitions and (2) published notices inviting applications in the&amp;nbsp;Federal Register.&amp;nbsp; For the first indicator, we finalized spending and policy decisions for each program in the early months of Q2.&amp;nbsp; These decisions are captured in an internal database, which can then be used to determine whether it is likely that ED will meet its goal.&amp;nbsp; As of Q4, all but two programs contributing to the APG have finalized their FY16 competitions.&amp;nbsp; Based on preliminary data, we have exceeded our goal for this year but are unable to report final numbers at this time.&amp;nbsp; For the second indicator, all programs running new competitions must publish a NIA in the&amp;nbsp;Federal Register.&amp;nbsp; In general, these notices are published by the end of Q2. As noted above, due to delays earlier in the year not all NIAs were published by the end of Q2; however, as of the end of Q3 all evidence-based competitions (save one, whose funding availability extends into FY17) were announced. The EPG, and anyone with an internet connection, can review each NIA on the&amp;nbsp;Federal Register&amp;nbsp;website.&amp;nbsp; When a program running an evidence-based competition publishes its NIA, we know we can count at least a portion of its funds available for new awards towards this APG.&amp;nbsp;After the NIAs publish, the next main milestone that we track is when awards are made. Most ED programs make their awards by the end of Q4, with a few exceptions that will make awards in the following fiscal year.

&amp;nbsp;

Because many of ED’s competitive grant programs involve offering professional development, IES offered a five-part webinar series on designing strong studies of professional development through the Regional Educational Laboratory Southeast. This was offered live in January 2016, just at the time when some applicants were designing their proposed evaluation activities. The archived webinars, agendas, and handouts are available on the IES website. In September 2016, IES launched a webpage on its website for evaluation-related technical assistance materials that were used to support i3 and FITW grantees (available here: http://ies.ed.gov/ncee/projects/evaluationTA.asp). The purpose of this webpage is to make these tools and materials widely available, including to applicants and grantees of other ED programs that require evaluations.

&amp;nbsp;

In addition, IES has implemented a robust strategy for managing WWC study reviews at scale and on an accelerated timeline. IES currently has five contracts that comprise the WWC investment, including a task order contract that allows for quick-turnaround reviews of studies that are submitted as part of grant applications. The WWC also is developing an online reviewer certification course, which may generate a greater number of individuals who are familiar with the WWC standards and capable of conducting study reviews.&amp;nbsp;IES is preparing enhancements to the WWC’s reviewed studies database and the Find What Works interface, two key features on the WWC website that allow practitioners and policymakers to search for evidence from effectiveness studies. The new Find What Works tool and the reviewed studies database portal will be released as part of a major WWC website overhaul, which is scheduled for launch in October 2016.&amp;nbsp; In addition, in early June 2016 IES launched a new online tool, RCT-YES, intended to help ED grantees design and conduct evaluations that meet WWC Evidence Standards.

&amp;nbsp;

The RCT-YES software package (Version 1.0) was released on June 14, 2016, and an updated version (Version 1.1) was released on July 25, 2016 that fixed a few bugs identified by users.&amp;nbsp; Through October 26, 2016, the RCT-YES website had 12,060 visitors (from all 50 states and DC, as well as international users), and 672 downloaded the software.&amp;nbsp; In the year leading up to the release, demonstrations of RCT-YES were conducted at six conferences, two foundations, and three agencies beyond ED (the Office of Management and Budget, the Department of Health and Human Services, and the Department of Labor).&amp;nbsp;&amp;nbsp;

&amp;nbsp;

To ensure that ED stays on track to meet our goal for the second metric, IES and the i3 program maintain internal databases with projected schedules for when grantees will release their evaluation findings.&amp;nbsp; Although i3 grantees are required to make their evaluation findings publicly available, they are not required to make them available in a way that most easily facilitates WWC review. Despite this, the i3 program staff continue to encourage their grantees to submit final reports to ERIC (where they remain public in perpetuity) and indicate that they are i3-funded studies. The i3 program and IES staff are also in regular communication about when evaluation findings have been made publicly available and are ready for WWC review.
</next_steps><progress_update>In Q4, the Department spent virtually all of its FY16 discretionary grant money. Two programs that impact this metric have funds available through part of FY17 (Investing in Innovation) or all of FY17 (Striving Readers), so the final calculations for this metric are not yet available. However, when considering just the programs that have already made their FY16 awards, the Department has surpassed its target for this metric.

&amp;nbsp;

Despite this success, the Evidence Planning Group (EPG) continued to grapple with complex issues, such as the sustainability of the Department's evidence review process, transitioning to a new elementary and secondary education law, and above all, whether the efforts we have made over the past several years to use evidence in more grant programs has actually changed program staff or grantee behavior. EPG is currently meeting at least once a week to delve deeper into these cross-cutting issues and has, so far, developed a draft process for how evidence reviews could work (in FY18 and later) in a way that incorporates ESSA's evidence requirements and alleviates burden on IES while still upholding the integrity of the review. In addition, planning for FY17 is underway. Program offices have submitted their spending plans for ED review assuming level-funding next year, and EPG has already made a list of possible programs that could run evidence-based competitions in FY17. EPG continues to work with program offices to flesh out evidence plans.

&amp;nbsp;

Regarding the second metric, the WWC continued to review studies for the i3-funded evaluations that have produced publicly available reports. To date, the WWC has reviewed the majority of i3-funded evaluations that are publicly available and reviews are underway for the remaining studies. As of Q4, we have exceeded our FY17 target for this metric.&amp;nbsp;
</progress_update><themes><theme>Education, Training, Employment, and Social Services</theme></themes><contributing_programs_and_other_factors>CONTRIBUTING PROGRAMS

ED is currently planning for the following programs to contribute to one or both metrics of this APG:

&amp;nbsp;

Striving Readers

TRIO – Talent Search

Charter Schools Program

Investing in Innovation Fund (i3)

Various Higher Education Act Title III Programs&amp;nbsp;

Various Office of Special Education Programs

National Professional Development

Innovative Approaches to Literacy

&amp;nbsp;

For additional programs see Appendix D&amp;nbsp;of the Department’s FY2015&amp;nbsp;Annual Performance Report and FY2017&amp;nbsp;Annual Performance Plan, available here:&amp;nbsp;http://www2.ed.gov/about/reports/annual/2017plan/2015-2017-apr-app-plan-...
</contributing_programs_and_other_factors></Agency_Priority_Goal><Strategic_Goals><Strategic_Goal><id>503</id><strategic_goal_header>Continuous Improvement of the U.S. Education System</strategic_goal_header><strategic_goal_statement>Enhance the education system’s ability to continuously improve through better and more widespread use of data, research and evaluation, evidence, transparency, innovation, and technology.
</strategic_goal_statement><strategic_goal_overview>Achieving the president’s 2020 college attainment goal will require better and stronger systems, powered by reliable and usable information that informs decision making, as well as by innovation.  Through this strategic plan, the Department aims to foster a culture of continuous systems improvement at the national, state, and local levels.  To achieve this goal, the Department will support robust and comprehensive data systems that produce timely, relevant, and understandable information while properly protecting student privacy; strategic use of research, evaluation, and  evidence in decision making at all levels; increased innovation; and effective and systemic use of technology.
 
The foundation for improving systemic capacity is an infrastructure that supports data-driven decision making.  Stakeholders must have access to relevant, useful data in a timely fashion, and the skills to better understand and make use of the data.  With relevant and actionable data and the ability to use it, policymakers and educators will be able to appraise how states, districts, schools, and students are currently performing; measure progress; pinpoint gaps; improve practice; better address student needs; and make sound decisions.  States are developing systems that will yield the valid, reliable data that are essential to achieving these purposes, but there is much more work to do.  The Department will continue helping states develop effective statewide longitudinal data systems, design voluntary common data standards to increase interoperability, and develop the capacity of institutions and staff to utilize data to improve teaching and learning.  These activities will help to generate an accurate picture of student performance and other critical elements, from early learning programs through postsecondary institutions and the workforce. At the same time, the Department will work through the government-wide Open Data Initiative to ensure that its own information and data are accessible to and useable by researchers, analysts, and developers in the general public.
 
Of course, the collection and use of data must be responsible and must appropriately protect student privacy.  Stewards and users of data must remember that data describe real people and ensure that systems protect the rights of those people. But there is no need to sacrifice data-driven decision making to protect student privacy, and the Department will help practitioners in the field ensure they are properly protecting privacy, and communicating with parents and students about the proper use and management of student data.
Systemic improvement also requires research and evaluation so that decision makers at the national, state, and local levels have reliable evidence to inform their actions. The Department aims to support research that will make a difference by giving states, districts, and schools the information and evidence they need to identify the effective practices they need to adopt. This research will also help them focus scarce resources on investments most likely to have the greatest impact and become more dynamic learning organizations.  In service of this, the Department will use its programmatic and research activities to incentivize the creation and use of evidence by stakeholders in the field. In addition, the Department will use evidence to inform its own policymaking and program development, and will work to communicate the findings of its research and evaluation activities to the field in an engaging and relevant way, further increasing their impact.
The goals and actions in this strategic plan sharpen the focus on the need to generate bold and creative solutions, and aim to support innovation in partnership with other federal partners and private organizations with related missions.  The Department will continue to stimulate innovations in education and provide funds needed to accelerate their design and adoption.  It also will work with Congress to fill a gap in the research and development landscape by establishing an advanced research projects agency for education to pursue breakthrough developments in ways not possible using existing Department mechanisms.
 
The Department’s vision for 21st-century learning also requires that schools have a 21st -century technology infrastructure, anchored around high-speed Internet. States, districts, and schools must have such infrastructure to incorporate cutting-edge methods for strengthening curriculum quality and delivery to meet more rigorous college- and career-ready standards; improving student access and engagement; developing comprehensive, formative, and summative assessment systems; and enhancing data management systems.
How will we measure success?  
 
The Department will use the following indicators to measure its success in enhancing the U.S. education system’s ability to continuously improve through better and more widespread use of data, research and evaluation, evidence, transparency, innovation, and technology.
 
 
Continuous Improvement of the U.S. Education System’s Indicators of Success
Increase number of states linking K–12 data with early childhood data
Increase number of states linking K–12 and postsecondary data with workforce data
Increase number of high-value data sets published through Data.gov or ED.gov websites
Decrease average time to close matters for the Privacy Technical Assistance Center (PTAC) and Family Policy Compliance Office (FPCO)
Increase number of new peer-reviewed, full-text resources added to the Education Resources Information Center
Increase number of reviewed studies added to the What Works Clearinghouse  database
Increase the percentage of select new[1] (non-continuation) competitive grant dollars that reward evidence
 dollars that reward evidence
Increase percentage of schools in the country that have Internet bandwidth speeds of at least 100 Mbps
What could hold us back?  What’s beyond our control?  (External Risk Factors)
 
Efforts to ensure that robust, integrated data systems are linked with early childhood, postsecondary, and workforce-level data will be constrained by the amount of time, financial resources, and support available to states to carry out this work.  State and local funding for data systems may be reduced due to continued budgetary pressures at all levels.  Moreover, wide variations in the various data systems present unique challenges for each state.  Some district data systems, for example, far surpass their own state’s data system.  Efforts to ensure that data systems lead to data-driven decision making may also encounter obstacles around privacy concerns that will need to be addressed.
 
The implementation of these strategies depends on the availability of resources for evaluation and research, which are typically more difficult to obtain than resources for services and program implementation.  In addition, many of the activities supporting this goal will be carried out by contractors and grantees, and thus are, in part, dependent on the capacity and quality of research expertise that exists outside the Department.  This variance in capacity may be more pronounced when it comes to interpreting and using evidence to drive decision making. Similarly, the evidence base regarding interventions in some areas is more robust than in others. And some grant programs focus on areas with less-developed evidence bases. At least in the short term, this could affect the amount of Department funding tied to the existence of certain levels of evidence for projects. Additionally, there are the perceived and real challenges states and districts face in implementing programs in ways that can support the strongest possible studies and evaluations of those programs. Some of these challenges can be addressed through Departmental technical assistance, but others may arise.
 
As reflected by Strategic Objective 5.2, one ongoing challenge is to protect privacy while increasing accessibility to relevant information. For its part, the Department will continue to vigilantly safeguard all personally identifiable information while also helping education agencies make useful information available. The Department also will help stakeholders develop best practices in protecting privacy while still driving data-driven decision making. But no system is perfect, and concerns about privacy may deter stakeholders from embracing the use of data, regardless of the actual level of risk presented.  Another risk lies in finding the right balance of information as too much information could make it nearly impossible to find meaning within data. To avoid this problem, the Department must work with stakeholders to prioritize information and to determine what is most likely to help families make choices for their children, help teachers improve student learning, and enable decision makers to improve education institutions.
 
Education funding faces tremendous budgetary pressure across the country, putting investment in new programs, strategies, or processes at risk.  In this time of decreasing budgets, technology infrastructure and other innovations might be seen as luxuries rather than as mission critical.  Because the private sector currently does not find investment in education innovation as attractive as investment in innovation in other sectors, there is limited private funding to help bridge the gaps.  And the promise of technology might be affected by variance in the capacity of states, districts, and schools to plan, implement, and support modern technology networks.
 
[1] “New competitive grant dollars that reward evidence” includes all dollars awarded based on the existence of at least “evidence of promise” in support of a project, per the framework in the Education Department General Administrative Regulations (34 CFR Part 75). Consideration of such evidence appears through: eligibility threshold (e.g., in the Investing in Innovation program); absolute priority; competitive priority (earning at least one point for it); or selection criteria (earning at least one point for it). The percentage is calculated compared to the total new grant dollars awarded, excluding awards made by the Institute of Education Sciences, the National Institute on Disability and Rehabilitation Research, and technical assistance centers, with some exceptions.
</strategic_goal_overview><strategic_goal_sequence>5</strategic_goal_sequence></Strategic_Goal></Strategic_Goals><Strategic_Objectives></Strategic_Objectives><Indicators></Indicators></data>